{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd07799ce36c48005a68e236bac859ea1c436d17ded1a929ff4f5154cd9b5438e8d",
   "display_name": "Python 3.9.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from some_functions import get_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "readers = pd.read_csv(r\"C:\\Users\\a814811\\OneDrive - Atos\\RecommenderSystem\\readers.csv\")\n",
    "readers = readers.rename(columns={\"id\":\"user_id\", \"art_id\":\"nzz_id\"})\n",
    "art_db = get_db(r'C:\\Users\\a814811\\OneDrive - Atos\\RecommenderSystem\\art_clean_wt_all_popularity.csv')\n",
    "art_db = art_db.loc[:,['nzz_id','author','department','popularity']] #skrócenie do potrzebnych rzeczy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_counts = readers[\"user_id\"].value_counts(sort=True)\n",
    "read_counts = read_counts.rename_axis(\"user_id\").reset_index(name=\"read_count\")\n",
    "\n",
    "# Biorę pod uwagę tylko użytkowników, którzy przeczytali minimum 5 artykułów\n",
    "min_read_count = 3\n",
    "read_counts = read_counts[read_counts[\"read_count\"] > min_read_count]\n",
    "\n",
    "readers = readers[readers[\"user_id\"].isin(read_counts[\"user_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = None\n",
    "readers_train, readers_test = train_test_split(readers,\n",
    "                                   stratify=readers[\"user_id\"], \n",
    "                                   test_size=0.20,\n",
    "                                   random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from popularity_model import *\n",
    "from model_evaluator import ModelEvaluator\n",
    "model_evaluator = ModelEvaluator(k_list = [5, 10, 15])"
   ]
  },
  {
   "source": [
    "### sprawdzam submodele i główny\n",
    "#### dla authora są mocno zaniżone wyniki, bo często nie rekomenduje nic a test tego nie uwzględnia, podobnie dla department (w mniejszym stopniu)\n",
    "#### merged i popularity działają normalnie (zawsze coś jest rekomednowane)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "999 users processed\n\nGlobal metrics:\n{'modelName': 'p_model', 'recall@5': 0.22400317460317462, 'precision@5': 0.04480063492063492, 'f1_score@5': 0.07466772486772487, 'ndcg@5': 0.1561412944064938, 'recall@10': 0.35361746031746033, 'precision@10': 0.035361746031746036, 'f1_score@10': 0.0642940836940837, 'ndcg@10': 0.19743235974972606, 'recall@15': 0.4404186507936507, 'precision@15': 0.029361243386243387, 'f1_score@15': 0.055052331349206354, 'ndcg@15': 0.21851573866821797}\n"
     ]
    }
   ],
   "source": [
    "# merged model\n",
    "p_model = Popularity_model_merge(art_db,readers)\n",
    "cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(p_model, readers, readers_train, readers_test)\n",
    "print(f'\\nGlobal metrics:\\n{cf_global_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = ['f1_score@5','f1_score@10','f1_score@15']\n",
    "# for it in x: print(f'{it}: {cf_global_metrics[it]}')\n",
    "results = pd.DataFrame([],\n",
    "        columns=['modelName',\n",
    "        'recall@5',\n",
    "        ' precision@5',\n",
    "        'f1_score@5',\n",
    "        'ndcg@5',\n",
    "        'recall@10',\n",
    "        'precision@10',\n",
    "        'f1_score@10',\n",
    "        'ndcg@10',\n",
    "        'recall@15',\n",
    "        'precision@15',\n",
    "        'f1_score@15',\n",
    "        'ndcg@15',\n",
    "        'weight'])\n",
    "v = list(cf_global_metrics.values())\n",
    "v.append((1,2,3))\n",
    "r1 = pd.DataFrame([v],\n",
    "        columns=['modelName',\n",
    "        'recall@5',\n",
    "        ' precision@5',\n",
    "        'f1_score@5',\n",
    "        'ndcg@5',\n",
    "        'recall@10',\n",
    "        'precision@10',\n",
    "        'f1_score@10',\n",
    "        'ndcg@10',\n",
    "        'recall@15',\n",
    "        'precision@15',\n",
    "        'f1_score@15',\n",
    "        'ndcg@15',\n",
    "        'weight'])\n",
    "results = results.append(r1,ignore_index=True)\n",
    "\n",
    "results.to_csv(\"res.csv\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "source": [
    "## Wizualizacja"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}